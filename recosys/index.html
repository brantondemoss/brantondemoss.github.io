<h1>Recosys Research</h1>
<h2>Definition of Problem</h2>
<p>Given the following:</p>
<ul>
<li>A set of customers, <img src="https://tex.s2cms.ru/svg/%5Cinline%20C" alt="\inline C" /></li>
<li>Corresponding customer metadata, <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Coverrightarrow%7BM_c%7D" alt="\inline \overrightarrow{M_c}" /></li>
<li>A set of banking products, <img src="https://tex.s2cms.ru/svg/%5Cinline%20B" alt="\inline B" /></li>
</ul>
<p>We wish to learn the following:</p>
<ul>
<li>A function <img src="https://tex.s2cms.ru/svg/%5Cinline%20f%3A(c%2C%5Coverrightarrow%7BM_c%7D)%20%5Ctwoheadrightarrow%20%20B%20%5Ctimes%20%5B0%2C1%5D" alt="\inline f:(c,\overrightarrow{M_c}) \twoheadrightarrow  B \times [0,1]" /> which assigns <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Cforall%20c%20%5Cin%20C%2C%5Cforall%20b%20%5Cin%20B" alt="\inline \forall c \in C,\forall b \in B" /> an associated probability <img src="https://tex.s2cms.ru/svg/P_b%20%5Cin%20%5B0%2C1%5D" alt="P_b \in [0,1]" /> that customer <img src="https://tex.s2cms.ru/svg/c" alt="c" /> would use the product <img src="https://tex.s2cms.ru/svg/b" alt="b" /> : <img src="https://tex.s2cms.ru/svg/%20%5Cinline%20f(%20(c%2C%5Coverrightarrow%7BM_c%7D)%20)%20%3D%20%5C%7B(b_i%2C%20P_%7Bb_i%7D)%20%7C%20b_i%20%5Cin%20B%5C%7D" alt=" \inline f( (c,\overrightarrow{M_c}) ) = \{(b_i, P_{b_i}) | b_i \in B\}" /></li>
<li>A function <img src="https://tex.s2cms.ru/svg/%5Cinline%20g%3A(c%2C%5Coverrightarrow%7BM_c%7D)%20%5Crightarrow%20%5Csigma(B)" alt="\inline g:(c,\overrightarrow{M_c}) \rightarrow \sigma(B)" /> which maps each customer-metadata pair to a subset of the permutations of <img src="https://tex.s2cms.ru/svg/%5Cinline%20%5Csigma(B)" alt="\inline \sigma(B)" /> : <img src="https://tex.s2cms.ru/svg/%5Cinline%20(b_0%2Cb_1%2C...%2C%20b_n)" alt="\inline (b_0,b_1,..., b_n)" />, <img src="https://tex.s2cms.ru/svg/n%20%5Cin%20dim(B)" alt="n \in dim(B)" />, where the ordering of the permutation corresponds to the likely customer preference ranking: <img src="https://tex.s2cms.ru/svg/g((c%2C%5Coverrightarrow%7BM_c%7D)%20)%20%3D%20(b_0%2Cb_1%2C...%2C%20b_n)" alt="g((c,\overrightarrow{M_c}) ) = (b_0,b_1,..., b_n)" /></li>
</ul>
<p>Note that given <img src="https://tex.s2cms.ru/svg/f" alt="f" /> we automatically get <img src="https://tex.s2cms.ru/svg/g" alt="g" /> by ranking according to the absolute probabilities given by <img src="https://tex.s2cms.ru/svg/f" alt="f" />.</p>
<p>Customer metadata primarily consists of transaction history in natural language and amount transacted, but may include additional metadata (demographics, social profile, etc…). We conjecture that transaction sequences contain more information and predictive power about customer state, and therefore susceptibility to recommendation. It is for this reason that we will want to go beyond classical collaborative filtering techniques which are unable to model sequential information.</p>
<h2>Recommendation Systems Background</h2>
<p>Recommendation systems primarily take two approaches to predicting relevant recommendations: collaborative filtering, and content based filtering.</p>
<p>Collaborative filtering works by calculating similarity metrics between users, then making recommendations according to similar user’s preferences.</p>
<p>Content based filtering works by finding similarities between items, and then predicting new items that a user will like according to the similarity content of their previously liked items.</p>
<p>In practice these systems work in very similar manners. The recommendation system designer decides on relevant user features (which items liked, time spent looking at X product, etc…) and defines a large (often sparse) matrix of these user-item interactions. The <a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf" title="Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix Factorization Techniques for Recommender Systems. Computer 42, 8 (Aug. 2009), 30–37.">former SOTA</a> method involves factoring this user-item matrix into separate lower dimensional User and Item matrices, which are then used to calculate new predictions.</p>
<p>Formally, let the sparse user-item rating matrix <img src="https://tex.s2cms.ru/svg/M" alt="M" /> with <img src="https://tex.s2cms.ru/svg/dim(M)%3D(u%2Ci)" alt="dim(M)=(u,i)" /> where <img src="https://tex.s2cms.ru/svg/(u%2Ci)" alt="(u,i)" /> denote the number of users and items respectively. Letting <img src="https://tex.s2cms.ru/svg/k" alt="k" /> = latent dimension, we approximately factor <img src="https://tex.s2cms.ru/svg/M" alt="M" /> with two lower rank matrices <img src="https://tex.s2cms.ru/svg/U%2CI" alt="U,I" /> such that <img src="https://tex.s2cms.ru/svg/M%3DUI" alt="M=UI" />, and <img src="https://tex.s2cms.ru/svg/U%5Cin%20%5CR%5E%7Busers%20%5Ctimes%20k%7D" alt="U\in \R^{users \times k}" />, <img src="https://tex.s2cms.ru/svg/I%20%5Cin%20%5CR%5E%7Bk%20%5Ctimes%20items%7D" alt="I \in \R^{k \times items}" />. Then we can fill in our sparse matrix (i.e. estimate user <img src="https://tex.s2cms.ru/svg/u" alt="u" />'s rating of item <img src="https://tex.s2cms.ru/svg/i" alt="i" /> by:</p>
<p align="center" style="text-align: center;"><img align="center" src="https://tex.s2cms.ru/svg/%20%5Ccentering%20m_%7Bu%2Ci%7D%20%3D%20%5Csum_%7Bj%3D0%7D%5E%7Bk%7DU_%7Bu%2Cj%7DI_%7Bj%2Ci%7D" alt=" \centering m_{u,i} = \sum_{j=0}^{k}U_{u,j}I_{j,i}" /></p>
<p>Interesting, <a href="https://sci-hub.se/10.1007/978-3-642-38844-6_3" title="What Recommenders Recommend – An Analysis of Accuracy, Popularity, and Sales
Diversity Effects">it has been shown</a> that when <img src="https://tex.s2cms.ru/svg/k%3D1" alt="k=1" />, this exactly corresponds to a <em>most-popular</em> ranking system with no personalization (the item with most recommendations is recommended). Increasing the latent dimension increases personalization, until a crossover when overfitting occurs. This method was used to win the <a href="https://en.wikipedia.org/wiki/Netflix_Prize">Netflix Prize</a>.</p>
<p>Unfortunately we can only represent numerical data using this method. If we can find a transformation from natural language transaction data to some latent space, we can apply similar techniques.</p>
<h2>Transaction Embeddings</h2>
<p>We want to find a transformation from natural language transactions into a latent space where &quot;distance&quot; encodes semantic similarity in the context of recommendation.</p>
<p>Luckily, there has been much progress in recent years on learning optimal natural language embeddings, from <a href="https://arxiv.org/pdf/1301.3781.pdf">single word vectors</a>, to <a href="https://arxiv.org/pdf/1502.06922.pdf">sentence vectors</a>, and <a href="https://arxiv.org/pdf/1706.03762.pdf">embeddings of arbitrarily structured language</a>. We can leverage these methods to learn optimal embeddings of natural language, in particular transactions as they relate to banking product recommendation.</p>
<p>Before we discuss language embeddings, we discuss the particular challenges of making banking product recommendations given only customer transaction and metadata.</p>
<h2>Absolute Probability Problems</h2>
<p>Our training data may not contain information about whether users were recommended the banking products they used, or to what extent (and <em>at what time</em>) they had information about relevant banking products.</p>
<p>Consider users <img src="https://tex.s2cms.ru/svg/U_1" alt="U_1" /> with transaction history <img src="https://tex.s2cms.ru/svg/H_1%3D(t_0%2Ct_1%2Ct_2%2Cb_1)" alt="H_1=(t_0,t_1,t_2,b_1)" /> and <img src="https://tex.s2cms.ru/svg/U_2" alt="U_2" /> with transaction history <img src="https://tex.s2cms.ru/svg/H_2%3D(t_3%2Ct_4%2Ct_5)" alt="H_2=(t_3,t_4,t_5)" />. Notice that <img src="https://tex.s2cms.ru/svg/U_2" alt="U_2" /> never used any banking product <img src="https://tex.s2cms.ru/svg/b_i" alt="b_i" />. Is this because the user decided not to use one, fully aware of their existence and relevance, or did they simply not know about the product or how it was relevant to them?</p>
<p>Without information about whether products have been recommended to users, or some kind of implicit knowledge information (had user searched for banking product <img src="https://tex.s2cms.ru/svg/b" alt="b" /> before?) we must consider our dataset incomplete and insufficient to learn absolute probability of use. Absolute probability of use is conditioned on knowledge about the product to be used, and without this knowledge we have an incomplete picture.</p>
<p>If we train a model to predict chance of using banking product given some transaction trajectories, we expect that probabilities learned this way will tend to underestimate the true value. Underestimation occurs due to the many examples of transaction trajectories that do not result in banking product use <em>because a recommendation was never made</em> (or the user simply did not know about the banking product even if it was relevant to them).</p>
<p>To address this issue, we will need to gather metadata about user knowledge of banking products (had they clicked on or searched for relevant banking product pages before), or about whether a recommendation had been made to the user.</p>
<p>On the long tail, we will want to train a recommender agent that incorporates user response into its recommendations. This seems like a reinforcement setting: We have an agent which makes actions (recommendations), transitions the environment state from a user not having been made a recommendation to having been made a recommendation, and receives a reward (+1 if user used recommended product, -1 otherwise). In addition to the recommendation policy (which product to recommend, or none, given some transaction sequence), we can output absolute recommendation probabilities <em>conditioned on a recommendation occurring</em>.</p>
<p>Designing, implementing, and training a full reinforcement agent will be a difficult task which we leave to future work. Until such a model becomes a viable business path, we recommend coarsely approximating &quot;absolute probabilities&quot; using some combination of relative rankings and average banking product usage. More on that later.</p>
<h2>Ranking</h2>
<p>We expect that learning relative bank product ranking will be a much more tractable problem.</p>
<p>There have been many recommender systems leveraging natural language processing advances. From simple <a href="https://arxiv.org/pdf/1708.05031.pdf">neural network approximations to the matrix factorization method</a>, to (RecVAE, Bert4Rec, etc…).</p>
<p>TODO:</p>
<ul>
<li>Talk about pros/cons of above-mentioned methods.</li>
<li>Classical Collab Filtering with word transaction embeddings learned from auto-encoder style net</li>
<li>Going from individual transaction embeddings to sequence, and attention based embeddings</li>
<li>Approximating users as average of their embeddings for classical filtering</li>
<li>Final end to end model</li>
<li>Discuss specifics of loss, regularization</li>
<li>Propose unsupervised pre-training of encoder on all transaction sequences</li>
<li>fine tune FC layers on encoder to predict banking product ranking</li>
<li>incorporate additional user meta-data in final FC layers of model</li>
</ul>
<p>Everything below this is old written from &gt; a week ago and will be completely re-written or deleted. These are basically notes to myself!</p>
<h2>Collaborative Filtering with Word2Vec</h2>
<p>We’ll start by examining the simplest possible model, keeping in mind that it builds towards more sophisticated approaches.</p>
<p>We first conjecture that there may be a correlation between distinct banking product use. That is, given that a customer is using banking product <img src="https://tex.s2cms.ru/svg/b_i" alt="b_i" />, can we infer a meaningful and predictive ranking of the remaining banking products <img src="https://tex.s2cms.ru/svg/b_j" alt="b_j" />?</p>
<p>Inspired by the seminal <a href="https://arxiv.org/pdf/1301.3781.pdf">Word2Vec</a> approach, we appropriate their method of ranking natural language word similarity to determine a &quot;similarity&quot; between banking products.</p>
<p>Using only banking product</p>
<p>With only banking product purchase histories, we can implement a simple recommender by utilizing a word2vec embedding style model. This is an extremely simple implementation that requires only one dataset.</p>
<p>Word2vec models work by first defining all words in your vocabulary. In our case this is the number of unique banking products. Each product is then represented as a one-hot encoded vector in the space of all products.</p>
<p>We can define a simple fully connected neural network with a <em>single</em> hidden layer. The width of the hidden layer defines the dimension of our latent space. The output of our neural network will be a vector of the same size as the input.</p>
<p>We define training data as such - say there is a banking product purchase history, <img src="https://tex.s2cms.ru/svg/(b_1%2Cb_2%2Cb3%2C...)" alt="(b_1,b_2,b3,...)" />. We define an integer hyperparameter called a <em>context window</em> <img src="https://tex.s2cms.ru/svg/%3DCW" alt="=CW" />, so that all pairs of banking products <img src="https://tex.s2cms.ru/svg/(b_i%2Cb_j)" alt="(b_i,b_j)" /> will constitute a valid training sample if <img src="https://tex.s2cms.ru/svg/b_i" alt="b_i" /> and <img src="https://tex.s2cms.ru/svg/b_j" alt="b_j" /> appear within <img src="https://tex.s2cms.ru/svg/CW" alt="CW" /> places of each other in any customer’s product purchase history. The set of all such pairs are the input/output training samples to train the neural network with.</p>
<p>After training we can find latent embeddings of any product by taking its hidden layer activation in the neural network. Then we simply find the cosine similarity with all other banking products’ latent embeddings to get a ranked list of recommended products. This method only produces a ranked list, not an absolute probability that a user will use a product.</p>
<h2>Seq2Vec with Transactions</h2>
<p>Following the work of <a href="https://arxiv.org/pdf/1607.01869.pdf">Scalable Semantic Matching of Queries to Ads
in Sponsored Search Advertising</a>, we can extend and improve the simple word2vec model by incorporating transaction history in addition to the banking product history. We treat them identically as</p>
<h2>Clustering</h2>
<p>Can use user embeddings as defined as (recent) averages of their transaction sequences to create clusters, then make recommendations in a k-NN approach. Can be used with full end to end model as hybrid approach.</p>
<h2>End to End Learning</h2>
<p><a href="https://iis.uibk.ac.at/public/papers/Ghazanfar-2012-InformSci.pdf" title="Kernel-Mapping Recommender System Algorithms">Kernel Map Paper</a></p>
<p><a href="https://dl.acm.org/doi/10.1145/1553374.1553452">Non-linear Matrix Factorization with Gaussian Processes</a></p>
<p><a href="https://arxiv.org/pdf/1912.11160v1.pdf">RecVAE: a New Variational Autoencoder for Top-N
Recommendations with Implicit Feedback</a></p>
