<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>The Complexity Dynamics of Grokking</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <link rel="icon" sizes="16x16 32x32" type="image/png" href="../../favicon.ico"><br />
  <meta property="og:image" content="http://brantondemoss.com/research/grokking/complexity_ours.jpg" />
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
</head>
<body>
<header id="title-block-header">
<h1 class="title">The Complexity Dynamics of Grokking</h1>
</header>
<center>
Branton DeMoss, Silvia Sapora, Jakob Foerster, Nick Hawes, Ingmar Posner
</br>
<a href="http://arxiv.org/abs/2412.09810">arXiv</a>·<a href="https://github.com/brantondemoss/GrokkingComplexity">code</a>
</center>
<p></br> <img src="fig1.png" /> <em>In grokking experiments, the
unregularized network’s complexity rises and remains high as it
memorizes the training data, never generalizing. The regularized
network’s complexity similarly rises, but as it groks and generalization
occurs, the complexity falls.</em></p>
<p>When can we say a neural network “understands” the task it has
learned? To answer this question, we study networks which suddenly
transition from memorization to perfect generalization: the “grokking”
phenomenon first reported by <a
href="https://arxiv.org/abs/2201.02177">Power et al</a>. We introduce a
new way to measure the complexity of neural networks based on <em>lossy
compression</em> and Kolmogorov complexity, and use this framework to
track the complexity dynamics of neural networks which grok. We find a
characteristic <strong>rise and fall of complexity</strong> in the
networks, which corresponds to memorization followed by
generalization.</p>
<h2 id="grokking">Grokking</h2>
<p>When training neural networks, we typically expect train and test
loss to decrease together. The grokking phenomenon occurs when there is
a large gap in training time between train and test loss going to zero.
When the network gets 100% accuracy on the training data but very low
accuracy on the test data, we say the network has “memorized” its
training data (as opposed to learning an explanation for the data which
<em>generalizes</em>). For example, if we train a small transformer to
do modular arithmetic, we see that after a few hundred training steps,
the model has fit the training data perfectly: <img
src="grokking.png" /> However, it does not generalize until around <span
class="math inline">\(10^5\)</span> training steps. This network is
trained with a small amount of weight decay for regularization. If we
don’t regularize the network at all, it never generalizes and stays in
the memorization phase indefinitely. The grokking phenomenon provides a
crisp example of a <em>generalization phase transition</em> in neural
networks. Studying this phase transition can provide insights into the
nature of generalization.</p>
<h2 id="complexity-proxies">Complexity Proxies</h2>
<p>Neural network complexity is often identified with model size,
i.e. parameter count, or with the total norm of the weights. However,
neither of these measures adequately capture the nature of model
complexity:</p>
<ol type="1">
<li>In the case of the norm of the weights—is a network whose weights
are all “10” more complex than a network whose weights are all “1”?
Intuitively, both of these networks are very simple, but if we only look
at the norm, we’re led to think networks with larger parameters are more
complex. There is a symmetry associated to re-scaling the weight
norms.</li>
<li>If we think of the parameter count as the network’s complexity, then
<em>network complexity can’t change throughout training</em>, which is
clearly incorrect. At initialization, the networks perform randomly. By
the end of training, they are able to predict the train data well, so
something is certainly changing throughout training.</li>
</ol>
<p>While these examples might feel contrived, it’s important to
understand how these proxy measures go awry. We fix these issues by
using a universal measure of complexity, the Kolmogorov Complexity, and
show how to approximate this measure appropriately for neural networks.
Using this measure, we find that there is a characteristic <strong>rise
and fall of complexity</strong> in the networks as they transition from
memorization to generalization.</p>
<h2 id="mdl-and-generalization">MDL and Generalization</h2>
<p>Occam’s Razor says that between equally good explanations, the
simplest one generalizes best. This intuitive principle can be made
sharp by unifying ideas from statistical generalization bounds and the
Minimum Description Length (MDL) principle. The MDL principle says that
the best model <span class="math inline">\(M\)</span> for some dataset
<span class="math inline">\(D\)</span> is the one which minimizes the
<em>sum</em> of the model’s complexity <span
class="math inline">\(C(M)\)</span> and the entropy of the data under
the model <span class="math inline">\(H(D|M)\)</span>: <span
class="math display">\[\begin{equation}
    \min_M H(D|M) + C(M)
\end{equation}\]</span> Intuitively, the better a model explains a
dataset, the more it reduces the apparent entropy of that dataset, <span
class="math inline">\(H(D|M)\)</span>. However, this comes at the cost
of specifying the model. You always have to specify a model to reduce
entropy, which takes information. We can always reduce the entropy to
zero by making the model be e.g. a lookup table of all the training
data—a rather trivial “model”. A model is good only if it reduces
entropy more than it requires information to specify. In the lookup
table case, we’ve just exchanged all the data entropy for model
complexity, with no net compression savings. Solutions like these are
called “memorizing” solutions, and we do not expect them to perform well
on unseen data. A good model compresses the data by reducing its entropy
while being itself as simple as possible, hence minimizing the
<em>sum</em> of these two terms.</p>
Across a number of fields of statistical machine learning, there are
generalization bounds which relate the expected test error to the
measured train error and the model’s complexity:
<p>
<center>
test error <span class="math inline">\(\leq\)</span> train error + model
complexity
</center>
</p>
<p>Notice that if the error terms are identified as entropies, as is
typically the case, models which minimize the MDL principle also
minimize these generalization bounds! Hence, models which best compress
the data are the ones which are expected to generalize best.</p>
<h2 id="complexity">Complexity</h2>
<p>The MDL shows how low-complexity models are preferred when
compressing data, and statistical generalization bounds show how models
which satisfy the MDL are expected to generalize best. But what is
complexity? The <strong>Kolmogorov Complexity</strong> <span
class="math inline">\(K\)</span> of a string s is defined to be the
length of the shortest program <span class="math inline">\(p\)</span>
which produces s: <span class="math display">\[\begin{equation}
    K(s) = \min \{ \texttt{len}(p) | \texttt{exec}(p) = s \}
\end{equation}\]</span> Complexity should have units of information,
since it appears summed with entropy in the previous equations. If our
program is defined as a binary string, then this quantity has units of
bits. Strings like <code>11111111…</code> are simple because they have a
short description (program) which produces them:
<code>print ‘1’ N times</code>. Strings like <code>100101011010…</code>
are complex because they lack regularity and cannot be described
compactly. We say a string is <em><a
href="https://en.wikipedia.org/wiki/Algorithmically_random_sequence">algorithmically
random</a></em> if it has no description shorter than itself, i.e. <span
class="math inline">\(K(s) \geq \texttt{len}(s)\)</span>.</p>
<p>Notice that this conception of complexity is intimately related to
compression: simple strings are those which are highly compressible,
whereas maximally complex (algorithmically random) strings are not at
all compressible. In fact, while we cannot compute the Kolmogorov
Complexity, we can upper-bound it by compression: if we compress our
string, then the compressed data with the decompressor is a program
which prints the original string. So the tighter we can compress the
data, the tighter a bound on its complexity we can produce. We cannot
know a string’s true complexity, even in principle, but we can get
tighter and tighter upper-bounds on how complex it is, by compressing
the string more and more.</p>
<h2 id="noise">Noise</h2>
<p>Compressing the weights of our neural network lets us bound its
complexity by the compressed filesize of the weight vector. However, a
problem with estimating complexity by compression is the presence of
noise in the network weights. Networks are randomly initialized and
trained with a stochastic process, so they contain a lot of random
information which prevents useful compression, and interferes with
complexity estimation. To fix this, we take inspiration from <a
href="https://en.wikipedia.org/wiki/Rate%E2%80%93distortion_theory">rate–distortion
theory</a>, which formalizes <em>lossy compression</em>. Proper lossy
compression requires the notion of a <em>distortion function</em>, which
measures the degree and kind of distortion we are willing to accept when
lossily compressing. For image compression schemes like JPEG, the
distortion function captures human visual perceptive similarity. JPEG
exploits the fact that humans are insensitive to high-frequency details
in images, and removes high-frequency information from the image
representation. This results in large savings in the compressed filesize
for minimal distortion according to a model of human perception. To
extend this idea to neural networks, we set the distortion function to
be equal to the <em>loss function</em>. Just like JPEG, we coarse-grain
the network weights, and achieve significant increases in compression
rates with little to no decrease in performance under the loss function.
Our compression scheme works as follows:</p>
<ol type="1">
<li>Coarse-grain the network weights <span
class="math inline">\(\theta\)</span> to get <span
class="math inline">\(\tilde{\theta}\)</span></li>
<li>Check that <span class="math inline">\(\left | \mathcal{L}(\theta,
D_{\text{train}}) - \mathcal{L}(\tilde{\theta}, D_{\text{train}}) \right
| &lt; \epsilon\)</span></li>
<li>If the distortion bound is not satisfied, adjust coarse-graining and
repeat. If distortion bound is satisfied, then return the complexity
upper bound <span
class="math inline">\(\texttt{gzip}(\tilde{\theta})\)</span></li>
</ol>
<p>We always search for the coarsest set of weights <span
class="math inline">\(\tilde{\theta}\)</span> that satisfy the
distortion bound, since we want the tightest possible upper-bound on the
complexity. We use quantization and low-rank approximation to
coarse-grain the weights, which we discuss more in the paper.</p>
<h2 id="results">Results</h2>
<p>Throughout training, we track the complexity of the networks by
coarse-graining and compressing their weights. The compressed filesize
of the coarse-grained weights is a tight bound on the network
complexity. Monitoring the complexity of the networks on the grokking
tasks, we observe that the complexity first rises as the networks
memorize their training data, and then falls as the networks generalize,
and a simple pattern/explanantion emerges in the network.</p>
<p><img src="complexity_ours.png" /> <em>Network complexity and train
and test accuracy vs training steps. As train accuracy goes to <span
class="math inline">\(100\%\)</span>, the complexity rises. As
generalization occurs and test accuracy approaches <span
class="math inline">\(100\%\)</span>, the complexity falls as the
networks learn a simple explanation of the data.</em></p>
<p>Without the coarse-graining procedure, naïvely compressing the weight
vectors would show a roughly flat line throughout training with
filesizes over an order of magnitude larger. The coarse-graining
procedure is crucial to remove noise and reveal the sensitive complexity
dynamics during training. If we plot the complexity for unregularized
networks which never generalize, we see that their complexity stays high
after memorization:</p>
<p><img src="complexity_none.png" /> <em>Network complexity for
unregularized neural networks which never grok. Their complexity remains
high after memorization.</em></p>
<p>Complexity is a dynamical quantity that is constantly changing as
networks learn different representations. We demonstrate that when
sudden generalization occurs during grokking, it corresponds to a rapid
fall in the complexity of the network. We know from statistical
generalization bounds that the representations which generalize best are
those which best explain the data <em>and</em> are simplest. A deeper
understanding of complexity can help us understand phase transitions in
learning systems, and the emergence of abstractions which generalize.
Using intensive measures like complexity can also help us predict the
generalization capabilities of models without reference to a test set,
which is critical as the range of model capabilities increases. For more
details, please see <a href="http://arxiv.org/abs/2412.09810">our
paper</a>!</p>
</body>
</html>
