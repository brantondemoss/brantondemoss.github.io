<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>What’s a Concept, and How to Find One</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">What’s a Concept, and How to Find One</h1>
</header>
<p><a href="https://xkcd.com/1425/"><img src="xkcdtasks.png" style="float: left; max-width: 25%;margin-right: 5%;"/></a></p>
<blockquote>
<p>In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they’d have the problem solved by the end of the summer. Half a century later, we’re still working on it.<br><a href="https://xkcd.com/1425">- xkcd 1425: “Tasks” alt text</a></p>
</blockquote>
<p>Why is it hard to write a program to identify pictures of birds? Funnily enough, it’s for the exact same reason the US Supreme Court struggled to define obscenity, finally settling on the now-infamous criteria “<a href="https://en.wikipedia.org/wiki/I_know_it_when_I_see_it">I know it when I see it</a>”. The difficulty, of course, is that human made categories are not exact, and exist only by agreement. We’ve cut arbitrary lines through the space of things which exist: what’s a fruit vs. a vegetable? A car vs. a truck? These lines blur - no exact definition can, in the end, be given.</p>
<p>Those interested in the philosophical side of this discussion will appreciate Scott Alexander’s <a href="https://slatestarcodex.com/2014/11/21/the-categories-were-made-for-man-not-man-for-the-categories/">The Categories Were Made For Man, Not Man For The Categories</a>. With recent advances in natural language processing and machine understanding, we’ll discuss how these problems are solved today, and how they can be used to build better search engines.</p>
<h2 id="are-words-concepts">Are words concepts?</h2>
<p>The problem of computational understanding of concepts shows up in everyday life most frequently through search: Google a question, and you’ll more than likely receive an answer. How do such systems work?</p>
<p>Traditional search algorithms for information retrieval work assuming a very straightforward principle: documents that contain terms found in queries are more likely to be relevant to the query than documents not containing those terms. So, to give a relevancy ranking to a document, we can simply compute how often a search query term is found in the document (and give a bonus if those words are rarely found in other documents). This way of relevancy ranking for search is nicely captured in the <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf">tf-idf</a> (term frequency - inverse document frequency) score:</p>
<p><span class="math display">\[\text{tf-idf} = \frac{f_{t,d}}{ \text{max}\{f_{t&#39;,d}:t&#39; \in d \}}\times\log\frac{N}{| \{ d \in D:t \in d\}|}\]</span> where <span class="math display">\[
\begin{aligned}
f_{t,d} &amp;= \text{the frequency of term}\,t\,\text{in document}\,d\\
D &amp;= \text{the set of documents}\\
N &amp;= |D|\text{, the number of documents in the corpus}\\
\end{aligned}
\]</span></p>
<p>Using something as simple tf-idf, you get pretty far. People learn to harness the system and develop their “google-fu”, choosing rare words that are likely to appear in the document they want. However, there are problems.</p>
<p>Consider someone searching for the term “flora”: maybe the document they were searching for instead contained the word “vegetation”. Extremely similar concepts, but the strings don’t match, so the system won’t recommend it. “Okay”, you think, “well we’ll just add all the known synonyms of a word and swap them out. And if we detect ‘not’ before a word we’ll search instead for the antonyms! And we’ll build up syntax graphs, and grammatical hierarchies and…”</p>
<p><img src="parsethicket.jpg" /> <em>A <a href="https://en.wikipedia.org/wiki/Parse_thicket">Parse Thicket</a></em></p>
<p>You’ve just gone down the feature engineering path. It’s admirable, and many have made some damn impressive tools with great effort - but it’s ultimately brittle, won’t generalize to other languages, and took a huge amount of time to design. Can we do better?</p>
<h2 id="are-vectors-concepts">Are vectors concepts?</h2>
<p>Instead of trying to encode every scrap of explicit knowledge and structure we can think of into a system, can we build a system that learns for itself how best to represent concepts?</p>
<p>This was the questions on the minds of researchers in 2013, when the seminal <a href="https://arxiv.org/abs/1301.3781">word2vec</a> paper was published. They created a basic system of this kind in a remarkably simple way:</p>
<p>Consider encoding a word into a vector that represents the word’s place in the dictionary, e.g. if there are 100,000 words in the english language, and if e.g. “apple” was the first word alphabetically, the <em>dictionary vector</em> for “apple” would be a 1 in the first place, followed by a zero everywhere else: <span class="math display">\[\text{apple} = [1,0,0,0,...]\]</span> Now if “zebra” was the final word in the language, we would represent it like <span class="math display">\[\text{zebra} = [0,0,0,...,1]\]</span></p>
<p>and so on for the rest of the words in the dictionary. Now, take as a linguistic hypothesis the following: words that appear near each other in text often carry related semantic meaning. For example, the word “apple” will probably appear near the words “pie” and “tree” and “fruit” much more often than it will appear near the word “chair”. If we can build a system that outputs “pie” when it’s fed “apple”, or outputs “ocean” when it’s fed “fish”, it must have learned at least at a superficial level how concepts are related.</p>
<p>With this linguistic hypothesis, we want to train a neural network to output dictionary vectors representing “tree” and “fruit” when we put in the dictionaryvector for “apple”. We’ll build a single hidden layer neural network that accepts a 100,000 dimension vector, learns a transformation to some intermediate hidden representation (which we’ll force to be much smaller in size, say 1,000 dimensions), and then another transformation back to 100,000 dimensions, which will represent another dictionary vector:</p>
<p><img src="autoencoder.png" /> <em>A single hidden layer neural network</em></p>
<p>By training this network on millions of examples of nearby words sourced from text, the intermediate, hidden representation in the network is forced to encode general semantic information about concepts learned from text. After training for many iterations, the word2vec authors showed a remarkable result: the hidden representation of “king”, minus the hidden representation of “man”, plus the hidden representation of “woman” equaled… the hidden representation of “queen”. The network wasn’t taught this, but it learned to embed these words into a <em>semantic space</em>, where related concepts were mapped near to each other. It was forced to learn this mapping to a semantic space through its training procedure!</p>
<p><img src="glove_embedding.jpg" /> <em>An example semantic embedding of concepts into vector space, showing only two dimensions</em></p>
<p>Using semantic vectors like these, we can make progress towards making our concept search better. Now even if a user wants to search for “flora” but a text uses the word “vegatation”, it’s no matter! Their semantic vectors should be nearby each other in semantic vector space. By comparing these vectors, we can rank concepts by similarity of <em>meaning</em>, and get around the brittle term-based search of yesteryear.</p>
<h2 id="give-the-people-what-they-want">Give the people what they want</h2>
<blockquote>
<p>“A human is not a device that reliably reports a gold standard judgment of relevance of a document to a query.”</br>― Hinrich Schütze, Introduction to Information Retrieval</p>
</blockquote>
</body>
</html>
