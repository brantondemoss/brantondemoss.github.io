<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>kata</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="style.css" />
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="love-letter-to-katago-or-go-ai-past-present-and-future">Love Letter to KataGo, or: <br> Go AI past, present, and future</h1>
<p><img src="katagame.png" /> <em>KataGo (B) vs LeelaZero (W)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></em></p>
<blockquote>
<p>In order to programme a computer to play a reasonable game of Go - rather than merely a legal game - it is necessary to formalise the principles of good strategy, or to design a learning programme. The principles are more qualitative and mysterious than in chess, and depend more on judgment. So I think it will be even more difficult to programme a computer to play a reasonable game of Go than of chess. <br>- I J Good, 1965<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</blockquote>
<p>There’s something magical about the game of Go. For thousands of years, it has captured the imagination of those who want to learn <em>what it is to learn</em>, to think about what thinking means.</p>
<p>With the recent advent of strong, open source Go AI that can beat top professionals, it’s worth tracing the history of the game, why it remained so difficult to beat humans for so long, and what the future of Go may hold.</p>
<h2 id="classical-ai">Classical AI</h2>
<blockquote>
<p>Looked at in one way, everyone knows what intelligence is; looked at in another way, no one does.<br> Robert Sternberg, 2000</p>
</blockquote>
<p>How we define AI has changed over time, older naive definitions were mostly concerned with capability on specific tasks, defining AI as</p>
<blockquote>
<p>The science of making machines capable of performing tasks that would require intelligence if done by humans.<br/> Minsky, 1968</p>
</blockquote>
<p>Definitions like these are inherently unstable, because as we build these computer systems and become normalized to their (at first) astonishing capabilities, we stop thinking of their task performance as demonstrating any kind of intelligence. Definitions like these leave AI in a sort of <a href="https://en.wikipedia.org/wiki/God_of_the_gaps">God of the gaps</a> situation.</p>
<p>Tying intelligence to performance in any single task, or even finite set of tasks, doesn’t seem consistent and informative. Some have proposed that intelligence is the ability to perform many tasks well, or the ability to solve tasks in a diverse range of environments<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Others claim that intelligence is the ability to acquire new skills through learning<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. More recently there have been proposals<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a> that intelligence is a measure of skill acquisition <em>efficiency</em>. Given two agents with the same knowledge and fixed training time on a novel task, the more intelligent agent is the one that ends up with better skills.</p>
<p>The most popular AI system of the last century was Deep Blue, a chess playing system designed by researchers at IBM. The system consisted of a hand-crafted board evaluation function, a tree search to maximise expected board state value given an adversarial opponent, and custom hardware designed to accelerate those operations, achieving speeds of around 100 million position evaluations per second.</p>
<p><img src="abpruning.png" /> <em>Alpha-beta pruning tree<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></em></p>
<p>Value functions measure the “goodness” of states (read: how likely they are to lead to victory). Creating meaningful evaluation functions is no small task - indeed, the Deep Blue evaluation function consisted of 8000 hand coded heuristics<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a>! Programmers got together with chess experts to assign value to various board states - rooks on the back rank, passed pawns, king safety, etc… All of these values were combined into a single number representing the total scalar “value” of that position, which tree search can then optimize for expected future value, given an opponent who attempts to minimize your value (<a href="minimax">minimax</a>).</p>
<p>With a well-tuned value function and powerful tree search to read ahead and find a value-maximising trajectory, Deep Blue managed a win over Garry Kasparov, the world chess champion, in 1997<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a>.</p>
<p>Deep Blue is an example of an “expert system” - one which has human expert knowledge encoded into it. It did not learn from its play, or generate novel heuristics or understanding - it maximised board state value according to the human-defined value function.</p>
<h2 id="complexity">Complexity</h2>
<p>Like chess, Go is a deterministic game of perfect information. There is no stochasticity, no hidden state.</p>
<p>Unlike chess in which there are on average around 35 legal moves to consider playing each turn, there are on average around 250 legal moves to consider in Go.</p>
<p>In tic-tac-toe, we can search the entire game tree, and easily find the optimal response for any position. xkcd nicely summarized this in an image:</p>
<p><img src="xkcd.png" /> <em>Perfect <span class="math inline">\(\times\)</span> strategy<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></em></p>
<p>Although it is in principle possible to create such a tree for Go since it is a finite game, the state space of Go is very large: the number of legal positions<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a> in Go is approximately <span class="math inline">\(2.1 \times 10^{170}\)</span>.</p>
<p>Since a game is a trajectory through legal board states (with some transition constraints), the number of possible games of Go is considerably larger. The number of unique games of Go has been bounded between <span class="math inline">\((10^{10^{104}},10^{10^{171}})\)</span> <a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a> <a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a>.</p>
<h2 id="intuition-reading">Intuition &amp; Reading</h2>
<p>Because the state space of Go is too large to be enumerated and searched through, players must learn to focus only on promising moves when considering possible game state trajectories (variations), in other words players must develop an <em>intuitive</em> sense of what moves might be good, and avoid wasting time on dubious possibilities. Defining such a value function turns out to be much more difficult for Go than for chess.</p>
<p>While intuition guides move selection, reading variations strengthens intuition using a form of self-argument: because Go is a <a href="https://en.wikipedia.org/wiki/Zero-sum_game">zero sum game</a>, move choice is necessarily conditioned on an adversarial opponent. Player’s goals are perfectly anti-aligned, so an optimal strategy can be constructed by considering maximising future state-value <em>given a minimizing opponent</em> (this logic is nicely encoded in the <a href="https://en.wikipedia.org/wiki/Minimax">minimax algorithm</a>).</p>
<p>Hand crafted value functions were not enough to solve Go, though. The search space is simply too large, and heuristics too difficult to define. One approach that saw some success was a modified tree search called Monte Carlo Tree Search (MCTS)<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a>. MCTS randomly samples legal moves from the current position, and rolls out the game tree all the way to the end, each time using a random move. The value of the initial move is related to the proportion of rollout trajectories that result in a won terminal state. Somewhat surprisingly, Go bots using MCTS were able to reach advanced amateur level (low-mid dan) play!</p>
<p>There is something deeply interesting in the fact that defining state values by evaluating <em>random</em> rollouts to the end actually provides a meaningful approximation of “true value”. It seems tautological when spelled out, but truly “good” moves really do have a greater proportion of trajectories leading to victory, and <strong>random sampling</strong> is enough to approximate their value.</p>
<h2 id="neural-networks">Neural Networks</h2>
<p>If the heuristics of board evaluation and move selection are so hard to program, so hard to even specify, how can humans play Go so well? Some experts can read many variations out very quickly, but nothing like the hundreds of millions per second of Deep Blue.</p>
<p>Human move selection intuition is <em>excellent</em>. At a glance, a very small number of moves stand out as worth considering. From the experience of many games of Go, we seem to be able to learn a sharp sense of which moves work, and which moves don’t. We also have the advantage of being able to read Go theory, which is the distilled experience of many others over millennia (incorporating symbolic knowledge into learning systems is an unsolved problem).</p>
<p>How can AI agents be given this excellent intuition? Convolutional neural networks!</p>
<p><img src="conv.gif" /> <em>Convolutional kernel (dark blue) applied to input (blue) to produce output (cyan)<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a></em></p>
<p>Briefly, convolutional neural networks are an example of a <a href="https://en.wikipedia.org/wiki/Artificial_neural_network">neural network</a> that use only <em>local connections</em> which are particularly adept at learning about and processing spatially-correlated features in images. The GIF above shows a learned convolutional filter sliding around an image, producing a lower-dimension representation. Typical networks contain millions of such learned parameters, and can perform a <a href="https://www.youtube.com/watch?v=b62iDkLgGSI">wide</a> <a href="https://www.youtube.com/watch?v=D4C1dB9UheQ">variety</a> <a href="https://www.youtube.com/watch?v=qWl9idsCuLQ">of</a> <a href="https://www.youtube.com/watch?v=HgDdaMy8KNE">tasks</a> in image processing.</p>
<p>As convolutional neural networks started to show promise in image recognition tasks<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a>, and since neural networks can approximate any function<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>, people began thinking about using them to estimate the value function, treating the board state encoding as an “image” input to the CNN. The idea is straightforward: given some board state and final game result pair, <span class="math inline">\((s,r)\)</span> train your CNN to predict <span class="math inline">\(r\)</span> from <span class="math inline">\(s\)</span>. Even better, given the same state <span class="math inline">\(s\)</span>, estimate the next move.</p>
<p>And so people started downloading hundreds of thousands of games of Go played online by strong amateurs and training CNNs to predict moves and win-rates. Agents playing from raw move prediction alone could outperform some of the weaker Go bots, but still struggled against the MCTS bots. Combining CNNs for move selection (called the policy) and value estimation (probability of winning from current state), and incorporating MCTS with the estimated policies and values to select optimal moves (i.e. instead of randomly sampling moves, we weight the sampling by the policy priors from the CNN, and instead of rolling out to a terminal state, we estimate the value of the current state from the value network), these prototype CNN bots started to outperform all others, but professional humans were still out of reach.</p>
<h2 id="alphago">AlphaGo</h2>
<p>Although MCTS improved the play of the trained CNNs, the networks were trained only on human games and had no means of improving beyond human knowledge. They could only weakly imitate humans.</p>
<p>To solve this problem, AlphaGo uses self-play and reinforcement learning to improve the policy and value estimations.</p>
<p><img src="reinforcement.png" /></p>
<p>Broadly, reinforcement learning agents take actions in an environment, receive rewards and observations from their environment, and learn to adjust their actions to maximise future rewards. In this case, the “environment” is a simulated game of Go, and the reward is the final result of the game (i.e. the rewards are sparse, and only received after many actions are made).</p>
<p>Because the MCTS in AlphaGo optimizes for maximal value (which measures probability of winning), by producing and training on games of self-play, the network can be further trained to produce better value predictions, and importantly, the policy can be trained <em>on the MCTS search values</em>, that is, we can train the policy CNN to output the final move-transition values found by the MCTS during self-play. Using this system of producing games of self-play, and training on their policy and value results, AlphaGo was able to continually improve, and finally reach superhuman performance.</p>
<p><img src="leesedol.jpg" /> <em><a href="https://deepmind.com/alphago-korea">AlphaGo vs Lee Sedol</a></em></p>
<h2 id="alphazero">AlphaZero</h2>
<p>AlphaGo succeeded in beating world champion Lee Sedol, finally giving computers the edge over humans. But the team at DeepMind wanted to push the method further.</p>
<p>In addition to the raw board state, AlphaGo’s inputs included the following for every evaluation:</p>
<p><img src="alphagofeatures.png" /> <em>Feature planes of AlphaGo<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></em></p>
<p>In a sense, there was still Go-specific knowledge that AlphaGo was “programmed with”, and the team wanted to see if a bot with zero game-specific knowledge could perform similarly.</p>
<p>In 2017 the team published the AlphaGo Zero paper<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a>, with three primary improvements:</p>
<ol type="1">
<li>The networks are trained solely from self-play reinforcement learning, starting from random play using no human data.</li>
<li>Only the black and white stone positions are used as input features to the networks.</li>
<li>The policy and value networks are combined into a single network with shared backbone, with shallow policy and value heads on top.</li>
</ol>
<p>With these changes, AlphaGo Zero far surpassed AlphaGo’s performance, getting massive increases in training efficiency from the combined policy and value net, and from using a ResNet-like architecture<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> instead of a fully convolutional network.</p>
<p>To measure relative playing strength of different agents, a commonly-used metric is <a href="https://en.wikipedia.org/wiki/Elo_rating_system">Elo rating</a>. While the full details of Elo rating (and how it corresponds to handicap stones) are beyond the scope of this article, briefly, Elo rating encodes relative probability of winning. Using the standard scales, for example, a 100 point rating difference encodes an expectation that the higher rated player has a 64% chance of beating their opponent; if the difference is 200, then the expectation is 76%.</p>
<p>There is a wonderful plot of Elo ratings of various bots from the AlphaGo Zero paper:</p>
<p><img src="elo.png" /> <em>Elo comparison of various computer Go programs</em></p>
<p>Note that the raw network (just playing top move recommended by policy net) strength is around ~3000, while the full AlphaZero bot (using the policy network + MCTS + value network) achieves a rating &gt; 5000. This gives us an idea of how much stronger the tree search and value estimation makes the raw network move intuition.</p>
<p>Going back to our earlier definition of intelligence as a measure of learning efficiency, it would have been excellent to see how the Elo strength as a function of self-play games changed from AlphaGo to AlphaGo Zero.</p>
<p>Finally the DeepMind team extended their AlphaGo Zero method to chess and shogi, removing all Go-specific aspects of the program (e.g. not generating additional training samples from the board’s <a href="https://en.wikipedia.org/wiki/Dihedral_group"><span class="math inline">\(D_4\)</span></a> symmetry), and published again, calling it AlphaZero.</p>
<h2 id="zero-explosion">Zero Explosion</h2>
<p>AlphaGo shook both the Go world and AI research community, but DeepMind largely left their work behind and moved on to other topics. With only the research papers to guide them, many started to re-implement AlphaZero.</p>
<p>As early as the first published paper on AlphaGo, many private companies, especially in China, S. Korea and Japan (where commercial Go products are viable) began to recreate AlphaGo/Zero. While these bots were helpful to those who could afford access, it wasn’t until open source bots became wide-spread that the Go community could fully take advantage their benefits.</p>
<p>The most well-known open source bot is <a href="https://zero.sjeng.org/home">Leela Zero</a>, a faithful re implementation of AlphaZero, which uses crowdsourced GPU compute to produce games of self-play and train the network. Leela Zero has been training since late 2017, and has produced about 20 million games of self-play as of May 2020.</p>
<p><img src="leelaelo.png" /> <em>Leela Zero Elo rating vs. number of games of self-play<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></em></p>
<p>As Leela Zero and other bots became available to the public for review and play, Go experienced a cultural shift unlike any that had come before. Suddenly everyone had access to superhuman playing advice, and could get opinions on variations in study from one of the strongest players of all time. While AlphaZero was a breakthrough for the AI community, Leela and the open source bots like it were the real godsend for the Go community. Rather than just mimicking AlphaZero’s moves, people could use them for in-depth review and study. World #1 Shin Jinseo reportedly brings an iPad with Leela Zero loaded up everywhere to review ideas and games. As AlphaZero and Leela Zero’s influence on the game meta took hold, researchers at Facebook noticed that <a href="https://ai.facebook.com/blog/open-sourcing-new-elf-opengo-bot-and-go-research/">players became stronger faster than anytime in history</a>!</p>
<p>While a great resource to the Go community, these Zero bots still had problems: they were expensive to train, taking months or years to achieve super-human performance with “normal” amounts of compute, they were <a href="https://github.com/leela-zero/leela-zero/issues/1482">surprisingly bad at ladders</a> (at first), inherited AlphaGo’s tendency to make slack moves when ahead, couldn’t play with variable komi, and played erratically in handicap games.</p>
<p>In a 2019 World AI Cup, Leela failed to podium, losing <span class="math inline">\(3^{rd}\)</span> place to HanDol, a Korean bot which would later play Lee Sedol for his final game as a professional. Disappointingly, the commercial bots destroyed the #1 open source bot Leela, likely due to vastly greater compute resources for training at their disposal. It is unclear what algorithmic differences, if any, the commercial bots have vs AlphaGo.</p>
<h2 id="yann-lecuns-cake-of-learning">Yann LeCun’s Cake of Learning</h2>
<p>“Godfather of AI”, co-inventor of convolutional neural networks and Turing laureate Yann LeCun wants to tell you about his cake.</p>
<p><img src="cake.png" /> <em>Slide from <a href="https://drive.google.com/drive/folders/0BxKBnD5y2M8NUXhZaXBCNXE4QlE">that one talk he’s always giving</a></em></p>
<blockquote>
<p>If intelligence is a cake, the bulk of the cake is self-supervised learning, the icing on the cake is supervised learning, and the cherry on the cake is reinforcement learning</p>
</blockquote>
<p>Yann’s point is that the bulk of information contained in things is “unstructured”. Reinforcement learning takes extremely low information density, e.g. a win-loss signal from a Go self-play game, and propagates that learning signal through many board states (e.g. training the value network to predict win rate from given board state). The signal-to-noise ratio there is not good, so reinforcement learning is <strong>extremely</strong> data-hungry.</p>
<p>Supervised learning is a little better: let’s say we want to build a CNN to classify images of dogs. Each training example contains a human-created label, which is much less noisy than a Go result, and is backpropagated through only the current image, a much stronger learning signal. Supervised learning generally requires fewer examples than reinforcement learning to achieve good performance.</p>
<p>Finally there’s what Yann calls “self-supervised” learning, in which “the system learns to predict part of its input from other parts of its input”<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a>. The idea is that the unstructured input data contains far more information than any supervised labels ever could, and so finding ways to cleverly predict parts of the input results in much better learning signal and eventual learnt representations.</p>
<p>A fun recent example of successful self-supervised learning is monocular depth estimation<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a>.</p>
<p><img src="packnet.gif" /> <em>Monocular depth estimation<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></em></p>
<p>It would be very useful to estimate pixel-accurate depth maps from monocular camera images<sup>[citation needed]</sup>. Humans cannot accurately label per-pixel depth maps, and LiDAR data is expensive to gather, so can we somehow get a network to estimate depth using only raw images as training samples?</p>
<p>It turns out, yes! By exploiting frame-to-frame consistent scene geometry and the fact that sequential video frames contain many of the same objects, we can have a network guess a depth and scene pose, use some geometry to transform that scene to another viewpoint, and back propagate pixel-wise photometric loss from the reconstructed and actual images to learn depth and pose accuracy. Using this method, networks can learn to predict accurate depth maps with only raw video input.</p>
<p><img src="monocdiagram.png" /> <em>Self-supervised monocular network diagram<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></em></p>
<h2 id="katago">KataGo</h2>
<p>In late 2017 <a href="https://github.com/lightvector">lightvector</a> began work on a Go project, an AlphaGo-style bot for personal experimentation. For those interested in the gritty details, I highly recommend people check out the original <a href="https://github.com/lightvector/GoNN">repository</a> to follow along with his experimentation. The project evolved into a genuine research effort, and became <a href="https://github.com/lightvector/KataGo">KataGo</a>.</p>
<p>Like AlphaGo, KataGo uses a CNN to estimate win rate (value) and move choice (policy), but it forgoes some of the Zero methodology of disincluding Go-specific information, instead including relevant features as input to the network, such as ladder and liberty status, amongst others. In particular, for <span class="math inline">\(b =\)</span> board width, a <span class="math inline">\(b \times b \times 18\)</span> tensor of:</p>
<table>
<thead>
<tr class="header">
<th style="text-align: center;"># Channels</th>
<th style="text-align: left;">Feature</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: left;">Location is on board</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: left;">Location has {own,opponent} stone</td>
</tr>
<tr class="odd">
<td style="text-align: center;">3</td>
<td style="text-align: left;">Location has stone with {1,2,3} liberties</td>
</tr>
<tr class="even">
<td style="text-align: center;">1</td>
<td style="text-align: left;">Moving here illegal due to ko/superko</td>
</tr>
<tr class="odd">
<td style="text-align: center;">5</td>
<td style="text-align: left;">The last 5 move locations, one-hot</td>
</tr>
<tr class="even">
<td style="text-align: center;">3</td>
<td style="text-align: left;">Ladderable stones {0,1,2} turns ago</td>
</tr>
<tr class="odd">
<td style="text-align: center;">1</td>
<td style="text-align: left;">Moving here catches opponent in ladder</td>
</tr>
<tr class="even">
<td style="text-align: center;">2</td>
<td style="text-align: left;">Pass-alive area for {self,opponent}</td>
</tr>
</tbody>
</table>
<p>is passed as input to the CNN, along with an additional input vector of some global state properties including ko and komi details.</p>
<p>KataGo makes a number of seemingly small changes to the AlphaGo/Zero system that add up to huge efficiency gains in learning, and welcome usability improvements for the Go community.</p>
<p>Like AlphaGo, KataGo is trained from scratch via self-play reinforcement learning. There are four major improvements to learning efficiency:</p>
<ol type="1">
<li><p>Playout cap randomization:</p>
<p>As noted in the KataGo paper, there is a “tension between policy and value training […] the game outcome value target is highly data-limited, with only one noisy binary result per entire game”, while the optimal policy training would use around 800 MCTS playouts per move. In other words, the value net would like more games to be played more quickly, but the policy net would like MCTS during self-play to go deeper to get better policy targets, so the there is tension between these two goals due to limited compute. To solve this issue, during self-play KataGo occasionally performs a “full search” of 600 playouts for move selection, but mostly only uses 100 playouts to finish games more quickly. Only the “full search” moves are used to train the policy network, but because there are more game results, the value net has more training samples.</p></li>
<li><p>Forced playouts and policy target pruning:</p>
<p>There is a classic trade-off in reinforcement learning between exploration and exploitation: should you use the knowledge you’ve learned to take optimal actions, or should you explore seemingly non-optimal moves to discover new behavior? KataGo attempts to solve this issue with <em>forced playouts</em>, where each child of the root that has received any playouts receives a minimum number of playouts. For more details, <a href="https://arxiv.org/pdf/1902.10565.pdf">read the paper</a>.</p></li>
<li><p>Global pooling:</p>
<p>A relatively simple improvement is seen in KataGo by introducing occasional global pooling layers, so that the network can condition on board areas that may be out of reach of the perceptual radius of the convolutional layers. Experiments with KataGo showed that this greatly improves later stages of training, “as Go contains explicit nonlocal tactics (‘ko’), this is not surprising”.</p></li>
<li><p>Auxiliary policy targets:</p>
<p>I think this is the most interesting change in KataGo, which shares similar ideas with those from self-supervised learning: training additional policy targets. Typically AlphaZero style bots only predict move policies and board state values (via win rate). Taking the idea from LeCun’s slide that learning can be improved with the addition of more training targets, KataGo attempts to predict more game outcomes than just policy and value. In particular, KataGo also predicts final territory control, and final score difference. Quoting from the paper:</p>
<blockquote>
<p>It might be surprising that these targets would continue to help beyond the earliest stages. We offer an intuition: consider the task of updating from a game primarily lost due to misjudging a particular region of the board. With only a final binary result, the neural net can only “guess” at what aspect of the board position caused the loss. By contrast, with an ownership target, the neural net receives direct feedback on which area of the board was mispredicted, with large errors and gradients localized to the mispredicted area. The neural net should therefore require fewer samples to perform the correct credit assignment and update correctly.</p>
</blockquote></li>
</ol>
<p><img src="territory.png" /> <em>Visualization of ownership predictions by KataGo<a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></em></p>
<p>As a result of these improvements, KataGo massively outperforms Leela Zero and Facebook’s ELF bot in learning efficiency, with a factor of fifty improvement over ELF:</p>
<p><img src="efficiency.png" /> <em>Relative Elo rating vs self-play cost in billions of equivalent 20 block x 256 channel queries (log scale)</em></p>
<p>In addition to these improvements, KataGo also directly optimizes for maximum score (with some caveats), mostly eliminating the slack moves found in other Zero style bots. KataGo also plays handicap games against weaker versions of itself during training, plays on multiple board sizes, and with variable komi and rulesets, so it is flexible under permutations of these game settings.</p>
<p>With all of these additional features, KataGo adds up to the most useful analysis tool yet made for Go, providing players with greater insight into the opinions of a superhuman Go agent.</p>
<p>KataGo is likely now the strongest open source Go bot available, recently topping the <a href="http://www.yss-aya.com/cgos/19x19/standings.html">CGS rankings</a> in all board sizes.</p>
<p>I highly recommend those interested check out the original <a href="https://arxiv.org/abs/1902.10565">KataGo paper</a> - it’s an extremely accessible read.</p>
<h2 id="future">Future</h2>
<p>In a <a href="https://www.youtube.com/watch?v=uPUEq8d73JI">recent interview</a>, the lead researcher from AlphaGo, David Silver, said that he expects AlphaZero style bots to continue improving for the next 100 years, that the skill ceiling of Go still has not come close to being reached. KataGo provides a picture of how improvements will continue to be made, and how value for human players can be added along the way. Who knows, maybe next-generation Go bots will incorporate language models and be able to explain their move choices in natural language.</p>
<p>Will KataGo incorporate games played against external opponents? Self-play has worked wonders, but an even greater diversity of ideas can be found by learning from external opponents. Professional Go players used to say that even God couldn’t give them a four stone handicap. As KataGo inches towards that barrier with wins over professional players with three stones, how far will handicaps be pushed? Can an agent trained for optimal self-play learn the kinds of aggressive strategies needed to win the most difficult handicap games? Is learning an “optimal” strategy given the complexity of Go realistic? Will we need to incorporate learning opponent behaviour models to exploit their specific weaknesses?</p>
<p>The future of Go and AI is exciting. Though bots have overtaken humans in skill, they haven’t left us behind - as long as we can continue to learn how to play the game better, as long as we think about how to get our bots to think better, Go and AI will continue to flourish together.</p>
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
       (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
       m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
       })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
       
       ga('create', 'UA-48874513-2', 'auto');
       ga('send', 'pageview');
       
</script>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><a href="http://www.yss-aya.com/cgos/viewer.cgi?19x19/SGF/2020/05/14/693137.sgf">KataGo vs. Leela Zero</a>: B+Resign<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2" role="doc-endnote"><p><a href="http://www.chilton-computing.org.uk/acl/literature/reports/p019.htm">I J Good: The Mystery of Go, 1965</a><a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3" role="doc-endnote"><p><a href="https://arxiv.org/abs/0706.3639">Legg and Hutter: A Collection of Definitions of Intelligence</a><a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4" role="doc-endnote"><p>Jose Hernandez-Orallo: Evaluation in artificial intelligence: from task-oriented to ability-oriented measurement. Artificial Intelligence Review, pages 397–447, 2017<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn5" role="doc-endnote"><p><a href="https://arxiv.org/pdf/1911.01547.pdf">Chollet: On the Measure of Intelligence</a><a href="#fnref5" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn6" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning">Wikimedia Commons: AB Pruning - Jez9999 / CC BY-SA</a><a href="#fnref6" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn7" role="doc-endnote"><p><a href="https://core.ac.uk/download/pdf/82416379.pdf">Campbell et al: Deep Blue</a><a href="#fnref7" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn8" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Deep_Blue_versus_Garry_Kasparov">Deep Blue vs. Kasparov</a><a href="#fnref8" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn9" role="doc-endnote"><p><a href="https://xkcd.com/832/">xkcd 832</a><a href="#fnref9" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn10" role="doc-endnote"><p><a href="https://tromp.github.io/go/legal.html">Tromp: Number of legal Go positions</a><a href="#fnref10" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn11" role="doc-endnote"><p>Lower bound: <a href="GoGamesNumber.pdf">Walraet: A Googolplex of Go Games</a><a href="#fnref11" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn12" role="doc-endnote"><p>Upper bound: <a href="https://tromp.github.io/go/gostate.pdf">Tromp and Farneback: Combinatorics of Go</a><a href="#fnref12" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn13" role="doc-endnote"><p><a href="https://www.remi-coulom.fr/CG2006/CG2006.pdf">Coulom: Efficient Selectivity and Backup Operators in Monte-Carlo Tree Search</a><a href="#fnref13" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn14" role="doc-endnote"><p><a href="https://github.com/vdumoulin/conv_arithmetic">Dumoulin and Visin: Convolution arithmetic</a><a href="#fnref14" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn15" role="doc-endnote"><p><a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">Krizhevsky et al.: ImageNet Classification with Deep ConvolutionalNeural Networks</a><a href="#fnref15" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn16" role="doc-endnote"><p><a href="https://en.wikipedia.org/wiki/Universal_approximation_theorem">Universal Approximation Theorem</a><a href="#fnref16" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn17" role="doc-endnote"><p><a href="https://www.nature.com/articles/nature16961">Silver et al: Mastering the game of Go with deep neural networks and tree search</a><a href="#fnref17" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn18" role="doc-endnote"><p><a href="https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ">Silver et al: Mastering the game of Go without human knowledge</a><a href="#fnref18" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn19" role="doc-endnote"><p><a href="https://arxiv.org/abs/1512.03385">He et al: Deep Residual Learning for Image Recognition</a><a href="#fnref19" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn20" role="doc-endnote"><p><a href="https://zero.sjeng.org/home">Leela Zero</a><a href="#fnref20" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn21" role="doc-endnote"><p><a href="https://twitter.com/ylecun/status/1123235709802905600?lang=en">Yann Lecun Twitter</a><a href="#fnref21" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn22" role="doc-endnote"><p><a href="https://arxiv.org/abs/1806.01260">Godard et al: Digging Into Self-Supervised Monocular Depth Estimation</a><a href="#fnref22" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn23" role="doc-endnote"><p><a href="https://github.com/tri-ml/packnet-sfm">Guizilini et al: 3D Packing for Self-Supervised Monocular Depth Estimation</a><a href="#fnref23" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn24" role="doc-endnote"><p><a href="https://github.com/tri-ml/packnet-sfm">Guizilini et al: 3D Packing for Self-Supervised Monocular Depth Estimation</a><a href="#fnref24" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn25" role="doc-endnote"><p><a href="https://arxiv.org/abs/1902.10565">Wu: Accelerating Self-Play Learning in Go</a><a href="#fnref25" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section>
</body>
</html>
