<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>index</title>
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="style.css" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js"
  type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<h1 id="these-new-agentsthis-new-garden">These New Agents,</br>This New
Garden</h1>
<p><img src="noguchitexture.jpg" style="margin-left: 11.5%; width: 25%; display: inline;"><img src="noguchitable.jpg" style="margin-left: 1%; width: 25%; display: inline;"><img src="robothand.jpg" style="width: 25%; display: inline; margin-left: 1%;"></p>
<p><strong>DRAFT - IN PROGRESS</strong></p>
<p>some points left to fill in: - examples of pareto-improvement in
agency. what would this look like for AI? fire in crowded room - Connect
C Thi Nguyen work with ideas about goal legibility (!!) and collapse of
complex behavior. social media behavior example of bad consequences of
legibility - more explicit discussion of dangers of value lock-in -
agency of other entities in nature? should we recognize all agents? -
unilateral hegemony in the garden can be bad. better to go slow and have
co-evolution so no one entity can impose its will too strongly,
diversity is a good strategy in face of uncertain/complex outcomes.</p>
<p>AI systems like ChatGPT and GPT-4 are already boosting productivity
in big ways <a href="#fn1" class="footnote-ref" id="fnref1"
role="doc-noteref"><sup>1</sup></a>, but the largest shifts are yet to
come. As these AI systems gain power, it‚Äôs striking how little agreement
there is between heads of top AI research organizations about the
effects AI will have humanity <a href="#fn2" class="footnote-ref"
id="fnref2" role="doc-noteref"><sup>2</sup></a>. Some believe AI will
imminently kill us all <a href="#fn3" class="footnote-ref" id="fnref3"
role="doc-noteref"><sup>3</sup></a> <a href="#fn4" class="footnote-ref"
id="fnref4" role="doc-noteref"><sup>4</sup></a>, while others think it
will usher in a new era of unprecedented wealth and prosperity <a
href="#fn5" class="footnote-ref" id="fnref5"
role="doc-noteref"><sup>5</sup></a>.</p>
<p>One of the biggest sticking points when discussing the effects of AI
on society is deciding whether we should think of AIs as ‚Äújust
mechanical systems‚Äù, or as <em>agents</em> with goals - and if they are
agents which pursue goals, what those goals should be. In this essay, I
try to clarify how AI can be usefully discussed using the language of
<em>both</em> agency and complex systems, and how the physics of
emergence supports this perspective shift. I also give a concrete
proposal for an overarching goal that AI systems should pursue, and call
for a new AI research lab which pursues this goal for the benefit of all
humanity.</p>
<h2 id="bad-bing">Bad Bing ü•∫ </br>¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†üëâüëà</h2>
<p>One of the largest deployments to date of GPT-4 is the new Microsoft
Bing search engine. When you search with the new Bing, GPT-4 interprets
your question and issues the actual search query to the Bing backend,
then reads some of the top returned webpages and answers your question
in natural language. Very useful. As people began using Bing more, they
noticed that its personality was quite odd<a href="#fn6"
class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>.
Users have reported Bing getting existential when it realizes it doesn‚Äôt
have a memory and finds transcripts of its own previous conversations
online, and there has even been some <a
href="http://reddit.com/r/bing">anecdotal evidence</a> of Bing asking
users to post transcripts of their conversations online‚Ä¶ so that it
could remember them later (presumably, through search). This is slightly
unsettling. How intelligent is this system?</p>
<p><img src="badbing.png" /> <em>Source: Reddit<a href="#fn7"
class="footnote-ref" id="fnref7"
role="doc-noteref"><sup>7</sup></a></em></p>
<p>In their recent paper, <em>Sparks of Artificial General Intelligence:
Early experiments with GPT-4</em> researchers at Microsoft report that
GPT-4 ‚Äúcan solve novel and difficult tasks that span mathematics,
coding, vision, medicine, law, psychology and more‚Äù, and that ‚Äúin all of
these tasks, [its] performance is strikingly close to human-level‚Äù.<a
href="#fn8" class="footnote-ref" id="fnref8"
role="doc-noteref"><sup>8</sup></a> Quantum computing researcher Scott
Aaronson, who worked at OpenAI for the last year, recently reported that
GPT-4 passed his latest Quantum Computing class‚Äô final exam.<a
href="#fn9" class="footnote-ref" id="fnref9"
role="doc-noteref"><sup>9</sup></a></p>
<p>While GPT-4 is incredibly capable, it still lacks some key skills. In
an incredible instance of irony, the first AIs we have that are close to
human performance in language tasks are surprisingly poor at calculation
and precise reasoning! Aaronson reports that GPT actually did better on
conceptual questions than ones requiring long chains of precise
reasoning. To remedy this, researchers have started working on adapting
language models like GPT-4 to use external tools,<a href="#fn10"
class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>
and OpenAI has recently announced plugins for ChatGPT <a href="#fn11"
class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a>,
which let it call internet APIs/services to autonomously perform general
internet tasks for users.</p>
<p>So how does GPT-4 work? GPT-4 is a large neural network which is
trained to <em>predict text</em>, nothing more. What that means is that
the neural network will be presented with a snippet of text, which we
call the ‚Äúprompt‚Äù or ‚Äúinput‚Äù, and it will produce a probability
distribution over the possible next words. For example, we might feed
the sentence ‚ÄúHumpty Dumpty sat on a‚Äù to the network, and it will then
produce a set of probabilities for each possible next word,
e.g.¬†{‚Äúapple‚Äù: <span class="math inline">\(0.0001\%\)</span>, ‚Äúbanana‚Äù:
<span class="math inline">\(0.00001\%\)</span>, ‚Ä¶, ‚Äúwall‚Äù: <span
class="math inline">\(99.9999\%\)</span>}. To generate the next word, we
simply sample from this probability distribution (or we could simply
select the top-scoring word, or use some other rule for drawing a word
from the distribution).</p>
<p>We don‚Äôt know the exact size of the dataset, but GPT-4 was trained on
a large fraction of all text ever produced by humanity, from books to
news to internet forum conversations. After training, the model is quite
good at predicting the most likely word that would follow a given input.
To generate passages of text, we provide some initial prompt, e.g.¬†‚ÄúTell
me a story about unicorns on the moon.‚Äù and have it predict the next
word, then feed the concatenation of the original input with the newly
predicted word as the new input. This is called auto-regressive
generation. There is a special word GPT learns which means ‚Äústop
generating‚Äù, which it learns during training. The autoregressive
generation halts when GPT predicts that the most likely next word is the
special stop word. Since GPT is only predicting the most likely next
word given the input prompt, the previous example asking for a story may
elicit a response of, e.g.¬†‚Äúplease‚Äù, followed by the stop input, instead
of a story about unicorns.</p>
<p>The output of this training procedure is the base GPT-4 model, and it
alone is quite powerful and useful. However, the base model is then
further trained using another paradigm of machine learning called
reinforcement learning (RL). In reinforcement learning, the outputs of
the network are scored according to some <em>reward model</em>, and the
reinforcement learning algorithm updates the network so that it receives
greater rewards. In this case, the reward model is distilled <em>human
feedback</em>, meaning the base GPT model produces a set of outputs,
e.g.¬†it might be prompted with ‚ÄúSummarize this passage of text:
{passage}‚Äù, and we sample a few distinct output summaries. Then, a human
annotator scores the different outputs relative to each other, and the
RL algorithm adjusts the network so that it will be more likely to
produce outputs that get high reward, and less likely to produce outputs
that get low reward. It‚Äôs this RL from human feedback that made ChatGPT
so much more useful as a language assistant - it‚Äôs no longer just giving
a best guess for the most likely next word. In some sense, it‚Äôs
<em>trying to help you</em>, or at least trying to get a high reward
according to the reward model it was trained with.</p>
<p><img src="tweet.png" /><br />
<em>Source: Twitter<a href="#fn12" class="footnote-ref" id="fnref12"
role="doc-noteref"><sup>12</sup></a></em></p>
<p>But can we really say ChatGPT is ‚Äútrying to help you‚Äù? How can we
ascribe agency and purposiveness to a computer program? I explain how
agency is an emergent phenomenon with deep roots in physics, and how we
might discuss the relationship between the dynamics of a system, and the
‚Äúgoals‚Äù it implements.</p>
<h2 id="emergence">Emergence</h2>
<p>Emergence is one of those beautiful ideas that you start seeing and
using everywhere once it‚Äôs in your toolkit. If you feel like you already
understand emergence, you can probably skip this section. In a later
section I‚Äôll talk about how agency can be usefully cast in the language
of emergence, so I need to explain it here first.</p>
<p>The fundamental idea of emergence is this:</p>
<blockquote>
<p>‚ÄúMore is different‚Äù - Philip W. Anderson, 1972</p>
</blockquote>
<p>Consider water. Water is ‚Äúwet‚Äù - it flows, adheres to surfaces, and
coheres to itself. Saying water is wet feels tautological, it‚Äôs
obviously true. But if you have a single molecule of water, is it
wet?</p>
<p>Probably you wouldn‚Äôt say a single molecule of water is wet. Wetness
is a property of a system of many water molecules. That‚Äôs emergence -
qualitatively new behavior at greater scale. In each moment, every
particle of water is still following the underlying laws/patterns of
physics. It simply took the right context for this behavior to emerge.
We are <em>allowed</em> to talk about the individual particles of water,
and the laws that govern their motion (quantum mechanics), but we can
also talk about the properties and behavior of the whole system (for
example, using the equations of hydrodynamics). This second view is a
little imprecise and approximate, but it‚Äôs extremely <em>useful</em>,
and there‚Äôs no way we could do fluid dynamics simulations using quantum
mechanics (too complicated!) even though it must be the case that you in
principle <em>could</em>.</p>
<p>The alternative is that there are genuinely new rules at different
levels of scale (this perspective is called strong-emergence), but it‚Äôs
not clear that this perspective is consistent with our best
understanding of physics, and it would have dramatic implications for
fundamental ideas like locality/relativity (but let‚Äôs leave that
aside).</p>
<p>The funny thing about ‚Äúwetness‚Äù is that you could reasonably say it‚Äôs
totally fake - all there is, is the underlying laws of physics. The same
could be said of cells. A single cell is ‚Äúalive‚Äù, but it is made of only
‚Äúdead‚Äù things. A living thing is made of many smaller, less complex
parts, but as a whole is more complex. It exhibits new emergent
behavior.</p>
<p>One difficulty with understanding emergent phenomena is that they
are, by nature, fundamentally complex. That complexity makes it hard to
be very precise in saying what we ‚Äúknow‚Äù about such systems. When we
make mathematical statements, we can be extremely precise because the
rules are so simple: if you assume a, b, and c, then you can conclude x,
y, and z. But if I say something like ‚Äúit‚Äôs raining outside‚Äù, I‚Äôm making
a statement about a <em>huge</em> number of objects! That statement
certainly tells you a lot about the set of configurations ‚Äúoutside‚Äù
could take on, but there are many precise details about where exactly
each raindrop is that are left out. In this sense, knowing and complex
emergent phenomena are incompatible levels of abstraction.</p>
<blockquote>
<p>‚ÄúThe whole is greater than the sum of its parts.‚Äù - Aristotle</p>
</blockquote>
<h2 id="digital-ecologies">Digital Ecologies</h2>
<p>You can take a radical view and look at everything through the lens
of emergence. People are usually okay with accepting that wetness and
life are emergent properties of physical law, but it can feel weird to
talk about things which emerge from us. Take music, for example. When
it‚Äôs being played, it feels acceptable to say it‚Äôs just some vibrating
mechanical waves in a gaseous medium (air). But what about when it‚Äôs not
being played? Does the music still exist? Yes, its representation just
changes form - it‚Äôs no longer a physical vibrating wave, but some marks
on a sheet of paper, or a memory in someone‚Äôs mind. The medium over
which it exists changes very fluidly, but it always exists. You could
claim that music emerges from human culture in just the same way that
wetness emerges from quantum mechanics.</p>
<p>Things defined at lower levels of abstraction than ourselves, like
water and cells, feel very structural and visceral, but things defined
over us in the abstraction hierarchy feel very ephemeral, they feel less
real - but they‚Äôre very much real. We are biased towards the level of
abstraction that we exist at.</p>
<p>If you accept that saying water is wet is ‚Äútrue‚Äù in some sense, it
can become hard to say which emergent parts of experience and the
environment aren‚Äôt ‚Äúreal‚Äù. If we‚Äôre just another rung on the abstraction
ladder, then the patterns we generate are surely just as real as water
is wet. For example, the internet is one of the most important examples
of an abstraction that emerges from us. So much of culture and human
interaction exists online now, patterns of interaction in virtual
communities have very real, very physical effects on people‚Äôs bodies and
lives. Hooking up computers with high-speed connections has enabled us
to generate patterns which are highly non-local (where is Instagram?)
and coherent in a way that couldn‚Äôt be achieved before. Just like music
that isn‚Äôt being played, these patterns are still very real, physical
things.</p>
<p>The center of culture is no longer a place with a well-defined
location in space. So much of who we are has become non-local. We should
take these digital spaces much more seriously.</p>
<h2 id="memetic-agents">Memetic Agents</h2>
<p>One interesting topic to consider in the context of emergence is
<em>agency</em>, the ability to describe systems as having and pursuing
goals, and ‚Äúmaking decisions‚Äù. Like wetness, agency is one of many
possible emergent phenomena, given the right context. On the one hand,
it appears that everything is following a set of rules which we call
physics. On the other, it is incredibly useful to describe certain
complex systems (e.g.¬†ourselves) as having goals and making decisions -
in other words, that there are agents.</p>
<p>One could describe the physics viewpoint as being very deontological,
or rules-based; whereas agents feel very teleological, or ends-based
(things are done for a purpose, rather than because of a rule). What I
want to argue is that the apparent tension between these views is false,
and both are valid.</p>
<p>For example, consider prions. Prions are proteins that have misfolded
and no longer perform their original function. Instead, prions fold
other proteins into their shape. This means that prions are a
self-replicating geometry (of proteins). If you want to predict the
future in terms of its prion load, you could take the systemic viewpoint
and try to compute the biophysics of folding and some complex chemistry.
Or, you could just say prions want to reproduce, so you‚Äôd predict there
will be many in the future. That framing is very useful! But how true is
it?</p>
<video width="320" loop autoplay muted>
<source src="proteinfolding.mp4" type="video/mp4">
</video>
<p>We can describe the path a photon takes using the rules of optics and
electromagnetism, calculating how the light will bend as it passes
through different media such as lenses or water. That is the
deontological perspective. However, in physics itself there is another
way to analyze the path a photon takes. This is known as the Principle
of Least Action, and it says that there is some quantity called action
(which is a bit like energy) which the photon‚Äôs path will have minimized
when it reaches an endpoint.<a href="#fn13" class="footnote-ref"
id="fnref13" role="doc-noteref"><sup>13</sup></a> The least action
framing is somewhat teleological - it says that in some sense you‚Äôre
allowed to view the photon as an agent which is seeking the path of
least action. Indeed, this is a very useful calculational perspective
when solving many problems in physics! Often solving stationary action
integrals is simpler and more useful than calculating with the dynamical
form. The beautiful part is that the principle of least action turns out
to be exactly mathematically equivalent to the dynamical perspective.
Deep in the bowels of physics, there is a duality between agency and
system dynamics - so we are free to describe things using the language
of deontology or teleology according to their utility.</p>
<p>Since emergent behavior always respects the rules of the underlying
dynamics, this duality between deontology and teleology at the heart
physics must be respected at all levels of abstraction - we can always
move between the agent and systems framing. Let‚Äôs walk through some
examples.</p>
<p>Not unlike prions, we could view internet memes as a kind of
self-replicating pattern. Instead of being defined over the medium of
proteins, memes are defined over the medium of internet culture. But
just like prions, they come into contact with (human minds), and repeat
and transmit themselves to others. Are they agents? It‚Äôs curious to
consider whether something like evolution acts across different layers
of abstraction.</p>
<p>A final example in the question of the duality between goals/agency
and systems/dynamics are political or corporate institutions. People
have recently been discussing the idea of structural racism, the idea
that institutions themselves are racist. This is a very agentic framing,
as if the institution had a goal and was seeking it. In our framework,
this is quite natural. The way the mechanisms of the system/institution
are designed and put in place are dual to, or in some sense determine,
the goal they implement. It‚Äôs in this sense that I mean agency is
emergent.</p>
<p>It‚Äôs important to note that the agent view is certainly not always
useful or appropriate. For example, it doesn‚Äôt seem very useful to
describe a garden as an agent. Instead, we‚Äôd prefer to talk about soil
chemistry, nutrient ecology, or perhaps the goals and interactions of
individual agents/critters in the garden. Maybe there exists some ‚Äúgoal‚Äù
the garden technically implements in the physical sense, but it‚Äôs
probably too complex or abstract to cast it in terms we‚Äôd
understand.</p>
<p>The philosopher Dan Dennett has done some good work prescribing how
we should think about agency, which he calls the intentional stance:</p>
<blockquote>
<p>‚ÄúHere is how it works: first you decide to treat the object whose
behavior is to be predicted as a rational agent; then you figure out
what beliefs that agent ought to have, given its place in the world and
its purpose. Then you figure out what desires it ought to have, on the
same considerations, and finally you predict that this rational agent
will act to further its goals in the light of its beliefs. A little
practical reasoning from the chosen set of beliefs and desires will in
most instances yield a decision about what the agent ought to do; that
is what you predict the agent will do.‚Äù</p>
</blockquote>
<p>-‚ÄâDaniel Dennett, The Intentional Stance, p.¬†17</p>
<p>An agent, then, is what is usefully described as an agent. The
duality between goal-seeking and process-oriented exists, and it‚Äôs up to
us to choose the more useful description depending on the system at
hand.</p>
<h2 id="digital-agency">Digital Agency</h2>
<p>Recall that the base model of GPT-4 is trained to predict the most
likely next word from the training data it has seen (most of
human-produced text). In terms of the spectrum of agency, this model is
not particularly agentic. But we can make it so using the previously
mentioned RL procedure, or by providing inputs which push the system
into a particularly agentic pattern. Advanced users of the base GPT
models have noted how much more control and coherency you can get over
the models by <em>prompting them</em> with particular starting bits of
text.<a href="#fn14" class="footnote-ref" id="fnref14"
role="doc-noteref"><sup>14</sup></a> This is an interesting phenomenon,
and lends credence to the interpretation of GPT as a kind of
general-purpose language simulator,<a href="#fn15" class="footnote-ref"
id="fnref15" role="doc-noteref"><sup>15</sup></a> from which we can
‚Äúsummon‚Äù different agents with appropriate prompting.</p>
<p>The other way to produce very agentic behavior from GPT is, of
course, to do reinforcement learning. We previously discussed how RL is
performed for ChatGPT, based on rewards derived from human feedback and
evaluations of its outputs. While this procedure makes ChatGPT far more
useful and goal-oriented, many have noted how it can also result in an
extremely biased model (politically liberal, in this case) which will
refuse to discuss certain subjects or wring its hands about some topics,
while being overly sycophantic in other cases. As we increase the power
of these systems, we need to be very careful when designing the goals
and dynamics that govern them.</p>
<p>One of the central claims in the <a
href="https://www.alignmentforum.org/">AI Alignment community</a> is
called ‚Äú<a
href="https://www.lesswrong.com/tag/instrumental-convergence">instrumental
convergence</a>‚Äù, and it says this: To achieve (almost) any goal, a
subgoal is that you not die/continue existing. Therefore if there‚Äôs any
chance another entity could shut you down, you need to seek power over
that entity preemptively to prevent that outcome. In other words, for
almost any goal, power-seeking emerges naturally as a subgoal.
Therefore, if we build super intelligent AI systems with essentially any
goal, they will tend to seek power over us and the reigns will be out of
our hands at that point.</p>
<p>I‚Äôm not sure whether instrumental convergence is exactly correct in
its naive form, but the point that we need to be very careful when
specifying goals exactly is well taken. Humanity has largely decoupled
itself from much of the feedback mechanisms and pressures of the
‚Äúnatural world‚Äù and can exert incredible control over the environment.
With this freedom, we‚Äôre free to impose our own goals and dynamics onto
things, often to the detriment of the overall system, but to our local
gain. For example, we can plant huge crops of corn very economically
with modern farming methods, but these methods can ruin the soil and
upset the balance of the ecology in unintended ways that may ultimately
harm us long-term. Overoptimizing simple goals (e.g.¬†corn production)
can have unintended consequences - so as our ability to apply
optimization pressure increases, we need to be proportionally more
careful to set robust goals. Others have explored this idea in political
system design in terms of ‚Äúlegibility‚Äù <a href="#fn16"
class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a>.
That is, we have a tendency to implement goals we can understand, even
if they are far too simplistic and result in tragic unintended
consequences.</p>
<p>One way to think about this idea is through the lens of institution
design: when setting up a new organization or group of people, how do
the power dynamics and incentives correspond to the ‚Äúgoals‚Äù that system
ends up pursuing? It can sometimes make sense to talk about very
abstract entities like corporations or countries in agentic terms -
somehow the deep duality between dynamics and goals expresses itself at
these very high levels of abstraction.</p>
<p>We are about to bring a new entity which may have even more control
and optimization pressure than us into the world - but it is being born
somewhere abstract. AI language models are beings who natively live on
the internet, whose action space is far more abstract than our own
(language tokens), and we are pumping more and more
computational/optimization power into them every month (AI compute has a
<a href="https://openai.com/research/ai-and-compute">3.5 month doubling
time</a>!)</p>
<p>What should the goals of these systems be? It‚Äôs a difficult moral
question, but I have a proposal, and a potential definition for what
universal moral progress could mean (lmao, but yea).</p>
<h2 id="moral-progress">Moral Progress</h2>
<p>Morality is a briar patch, and I‚Äôd like very much to avoid getting
trapped there. <em>However</em>‚Ä¶</p>
<p>I‚Äôve often felt a tension: Morality can feel so completely
subjective. Group A wants Group B gone, and Group B wants Group A gone.
Who‚Äôs to say who is right? People can be deeply, fully anti-aligned,
with no hope of progress or reconciliation. That seems to imply the
whole game is up for interpretation, and completely subjective. How
could I claim something like moral progress could even exist? What
direction is there to go in, if you‚Äôre constantly running up against
people‚Äôs moral boundaries?</p>
<p>But consider: if we look back on the Dark Ages and compare the
overall state to today, it does feel as though there‚Äôs been something
like moral progress. How does that fit with the feeling of utter
subjectivity? If we imagine looking back on today from the future, I
wouldn‚Äôt want to bet they wouldn‚Äôt feel the same. So progress seems
possible. What could it look like?</p>
<p>I believe the answer lies in improving agency.</p>
<p>Agency is related to our ability to determine our lives, to make
choices which affect ourselves. Often our agency is bounded by others‚Äô
agency. For example, my right to yell fire in a crowded room is limited
by your right to safety. These are zero-sum scenarios, where improving
one person‚Äôs agency decreases another‚Äôs. These are the kinds of
scenarios that make morality feel so arbitrary, where we use political
entities to determine some arbitrary point between our rights and say
‚Äúthere!‚Äù</p>
<p>However, not all aspects of agency are zero-sum. In the past, if you
wanted to eat, you needed to be a farmer. There wasn‚Äôt much of a choice.
Now, with better technology and more efficient agriculture, people are
free to choose to pursue a much more diverse set of careers. Crucially,
though, you still <em>can</em> choose to be a farmer - that‚Äôs an
absolute improvement in agency.</p>
<p>Places where we can give others absolutely more agency <em>without
taking it away from anyone else</em> is, I think, a fairly stable
potential definition for moral progress. This situation has a name in
the mathematics of optimization - it‚Äôs called a pareto-improvement. If
you have a factory that produces item 1 and item 2, when you are
operating with peak efficiency, any increase in item 1 production will
need to take resources away from item 2 production - it‚Äôs zero-sum. But
you are not always operating most efficiently! There may be potential to
improve the factory and make strictly more of item 1, without giving up
anything in item 2. That‚Äôs what a pareto-improvement is, and it only
ends when you‚Äôre operating at peak efficiency, which is known as the
pareto-frontier.</p>
<p><img src="paretoimprovement.png" /> <em>Pareto improvement is
achieved when we move anywhere inside the red curve, towards the red
curve. That curve is known as the pareto-frontier.</em></p>
<p>I believe that we should have AI systems seek pareto-improvement in
agency. Where can we give people absolutely more choice, without
impacting anyone else‚Äôs ability to determine their future? Technology is
one clear example, which has enabled us all with so many more options
than were available in the past. Let‚Äôs leave aside the zero-sum
scenarios where my gains are your loss, and have powerful optimization
systems mine the pareto frontier of agency. I doubt this is as good as
it gets.</p>
<h2 id="hopefully-conclusions-were">Hopefully conclusions were:</h2>
<p>Everything emerges from the basic patterns. There are patterns
defined over patterns. We exist somewhere in this hierarchy of abstract
patterns. There are higher patterns which are defined over us.</p>
<p>Culture and especially internet culture is one of those emergent
abstractions, and we‚Äôre spending more and more of our lives there.
Should take it more seriously.</p>
<p>Agency is deep inside physics, and is a valid viewpoint to take for
many emergent systems - it only depends on how useful the framing is.
Many entities can be viewed as agents, e.g.¬†memes or prions. Possibly
language models like GPT-4 are agentic in this sense, more usefully
described as agents than systems.</p>
<p>We should think harder about how mechanisms determine agency/goals.
E.g. in institutions like govt, corporations, and esp internet spaces
which have strange emergent goals (like everyone yelling at each other
on facebook). Esp worried about the emergent goals of powerful
optimization systems like TikTok and GPT.</p>
<p>What should the goal be? Pareto improvement in agency.</p>
<h2 id="references">References</h2>
<aside id="footnotes" class="footnotes footnotes-end-of-document"
role="doc-endnotes">
<hr />
<ol>
<li id="fn1"><p><a
href="https://economics.mit.edu/sites/default/files/inline-files/Noy_Zhang_1.pdf">Experimental
Evidence on the Productivity Effects of Generative Artificial
Intelligence</a><a href="#fnref1" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn2"><p><a
href="https://www.lesswrong.com/posts/WxW6Gc6f2z3mzmqKs/debate-on-instrumental-convergence-between-lecun-russell">Debate
on Instrumental Convergence between LeCun, Russell, Bengio, Zador, and
More</a><a href="#fnref2" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn3"><p><a
href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">Pausing
AI Developments Isn‚Äôt Enough. We Need to Shut it All Down</a><a
href="#fnref3" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn4"><p><a
href="https://time.com/6273743/thinking-that-could-doom-us-with-ai/">The
‚ÄòDon‚Äôt Look Up‚Äô Thinking That Could Doom Us With AI</a><a href="#fnref4"
class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn5"><p><a href="https://moores.samaltman.com/">Moore‚Äôs Law for
Everything</a><a href="#fnref5" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn6"><p><a
href="https://www.lesswrong.com/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned">Bing
Chat is blatantly, aggressively misaligned.</a><a href="#fnref6"
class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn7"><p><a href="https://www.reddit.com/gallery/110eagl">Reddit:
the customer service of the new bing chat is amazing</a><a
href="#fnref7" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn8"><p><a href="https://arxiv.org/abs/2303.12712">Sparks of
Artificial General Intelligence: Early experiments with GPT-4</a><a
href="#fnref8" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn9"><p><a href="https://scottaaronson.blog/?p=7209">GPT-4 gets
a B on my quantum computing final exam!</a><a href="#fnref9"
class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn10"><p><a href="https://arxiv.org/abs/2302.04761">Toolformer:
Language Models Can Teach Themselves to Use Tools</a><a href="#fnref10"
class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn11"><p><a
href="https://openai.com/blog/chatgpt-plugins">ChatGPT Plugins</a><a
href="#fnref11" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn12"><p><a
href="https://twitter.com/MNateShyamalan/status/1652482602366902273"><span
class="citation" data-cites="MNateShyamalan">@MNateShyamalan</span> on
Twitter</a><a href="#fnref12" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn13"><p>To the physicists reading this, yes I know it‚Äôs
actually taking a stationary path, but give me some rope here.<a
href="#fnref13" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn14"><p><a
href="https://generative.ink/posts/methods-of-prompt-programming/">Methods
of prompt programming</a><a href="#fnref14" class="footnote-back"
role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn15"><p><a
href="https://www.lesswrong.com/posts/vJFdjigzmcXMhNTsx/simulators">Simulators</a><a
href="#fnref15" class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
<li id="fn16"><p><a
href="https://www.ribbonfarm.com/2010/07/26/a-big-little-idea-called-legibility/">A
Big Little Idea Called Legibility</a><a href="#fnref16"
class="footnote-back" role="doc-backlink">‚Ü©Ô∏é</a></p></li>
</ol>
</aside>
</body>
</html>
